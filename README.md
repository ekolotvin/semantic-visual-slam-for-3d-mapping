# Integration of Semantic Segmentation into Visual SLAM
![11](https://github.com/user-attachments/assets/2cf86d77-cca1-4169-b44c-926d316dd18f)
![Снимок экрана 2026-02-28 010250](https://github.com/user-attachments/assets/2525095e-fda9-4f1d-9fc8-0d6ff1241c7a)
## В этом проекте модель семантической сегментации интегрируется в пайплайн Visual SLAM для построения трёхмерной семантически размеченной сцены.
Для каждого входного кадра:<br>
1. Предсказывается семантическая маска;<br>
2. Маска интегрируется в SLAM;<br>
3. Строится 3D карта сцены с семантическими метками.

## Обзор проекта
Проект разделён на две основные части:

<ins>Часть 1 — Семантическая сегментация:</ins><br> 
Обучение и сравнение моделей DeepLabV3+ на различных дорожных датасетах.<br>
Используемые дасатесы:<br>
1) [The KITTI Vision Benchmark Suite (KITTI: Semantic Segmentation Evaluation)](https://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015);<br>
2) [Berkeley Deep Drive (BDD100K: 10K images + Segmentation)](http://bdd-data.berkeley.edu/download.html);<br>
3) Объединенный датасет (KITTI + BDD100K).<br>

<ins>Часть 2 — Интеграция в Visual SLAM:</ins><br>
Применение финальной модели сегментации для построения 3D сцены с семантической разметкой;<br>
Для реконструкции трехмерной сцены использован датасет<br>
[The KITTI Vision Benchmark Suite (Visual Odometry / SLAM Evaluation 2012)](https://www.cvlibs.net/datasets/kitti/eval_odometry.php).

## 1. Семантическая сегментация
**Датасет KITTI.**<br> 
Использовано 200 размеченных изображений;<br>
Выделено 10 унифицированных классов:<br>
*road, sidewalk, building, vegetation, sky, car, truck, bus, person,pole, traffic sign, terrain*;<br>
Для согласованности между датасетами был выполнен remapping классов.

<ins>Разбиение:</ins><br>
160 — обучение;<br>
40 — валидация.<br>

<ins>Обучение kitti_model</ins>.<br> 
Архитектура: DeepLabV3+;<br> 
Дообучение классификационного слоя;<br> 
50 эпох обучения;<br> 
Лучшая *kitti_model* сохранена по метрике mIoU на валидации;<br> 
После обучения проведён инференс на тестовых изображениях KITTI Dataset.

**Датасет BDD100K (10K images).**<br>
Использовано 10 000 размеченных изображений;<br>
После фильтрации и remapping оставлены те же 10 классов, что и в KITTI Dataset.<br>

<ins>Разбиение:</ins><br>
5500 — обучение;<br>
1000 — валидация;<br>
1500 — тест.

<ins>Обучение bdd_model</ins>.<br>
Инициализация DeepLabV3+;<br>
Дообучение классификационного слоя;<br>
50 эпох обучения;<br>
Лучшая *bdd_model* сохранена по метрике mIoU на валидации.<br>

<ins>Оценка переносимости (Cross-Domain)</ins>.<br>
Для анализа обобщающей способности *kitti_model* протестирована на тестовой части BDD100K;<br>
*bdd_model* оценена на своём тестовом наборе.<br>
Эксперимент демонстрирует влияние domain shift между датасетами.

**Объединённый датасет.**<br>
KITTI и BDD100K были объединены в единый набор данных.<br>

<ins>Разбиение:</ins><br>
6560 — обучение;<br>
820 — валидация;<br>
820 — тест.

<ins>Обучение general_model</ins>.<br>
Инициализация DeepLabV3+;<br> 
Дообучение классификационного слоя;<br>
50 эпох обучения;<br>
Сохранена лучшая *general_* по метрике mIoU на валидации.

**Сравнение моделей и выбор финальной модели.**<br>
На тестовой части объединённого датасета были оценены:<br>
*kitti_model*(mIoU = 0,468), 
*bdd_model*(mIoU = 0,776), 
*general_model*(mIoU = 0,673)<br>
Критериями выбора *final_model* являлись высокая метрика mIoU,а также качество визуального поведения на последовательностях изображений датасета KITTI Odometry.<br>
В качестве финальной модели была выбрана *general_model*, так как её визуальное качество на этапе inference заметно выше, а значение метрики mIoU лишь незначительно уступает *bdd_model*.<br>

*bdd_model* (inference):
![bdd_model](https://github.com/user-attachments/assets/ccc3534b-1b7d-48d2-8453-8ec2f54acbfd)

*general_model* (inference):
![general_model](https://github.com/user-attachments/assets/504eb55c-bdcc-4d05-8b6e-fbea62057e07)
## 2. Интеграция в Visual SLAM
**Построение 3D семантической карты**<br>
Для каждого кадра:<br>
<ins>Предсказывается семантическая маска.</ins><br>
RGB-изображение подаётся в обученную *final_model*;<br>
На выходе получается маска, где каждому пикселю присвоен класс.<br>

<ins>Преобразование LiDAR-точек в систему камеры.</ins><br>
LiDAR измеряет трёхмерные точки в своей собственной системе координат;<br>
Чтобы совместить их с изображением, точки переводятся в систему координат камеры с помощью калибровочных параметров *calib*;<br>

<ins>Облако лидарных точек проецируются на изображение.</ins><br>
Теперь для каждой 3D-точки можно определить её положение в пространстве и соответствующий класс из семантической маски.

<ins>Присвоение семантики 3D-точкам.</ins><br>
После проекции для каждой 3D-точки берётся класс пикселя, в который она попала;<br>
Точке присваивается цвет в соответствии с этим классом.<br>

Таким образом, стандартная геометрическая реконструкция сцены дополняется семантической информацией.

**Построение глобальной 3D карты**<br>
<ins>Чтобы построить полную карту сцены:</ins>

  1. Используется оценка позы камеры из SLAM;<br>
  2. Все локальные облака точек переводятся в единую глобальную систему координат;<br>
  3. Облака последовательно объединяются.<br>

<ins>В результате получается глобальная 3D карта поверхности сцены с семантической разметкой:</ins>

![1](https://github.com/user-attachments/assets/7b0e49f2-b7bd-4c24-b12e-a97eb3bb9766)<br>




